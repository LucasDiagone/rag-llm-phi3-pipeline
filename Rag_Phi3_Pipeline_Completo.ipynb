{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìò Projeto RAG com LLM Open-Source (Phi-3)\n",
        "\n",
        "Este notebook apresenta um **pipeline completo de Retrieval-Augmented Generation (RAG)** utilizando um **LLM open-source (Phi-3 Mini)**.\n",
        "\n",
        "O objetivo √© demonstrar, de forma did√°tica, como combinar **m√∫ltiplas fontes de dados**, **busca sem√¢ntica** e **gera√ß√£o de texto** em um √∫nico fluxo.\n",
        "\n",
        "---\n",
        "### üß† O que voc√™ ver√° neste notebook\n",
        "- Carregamento e uso de um LLM open-source\n",
        "- Ingest√£o de dados locais e da web\n",
        "- Chunking e embeddings de texto\n",
        "- Busca sem√¢ntica com FAISS\n",
        "- Gera√ß√£o de respostas baseada em contexto (RAG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Etapa 1 ‚Äî Carregamento do LLM (Phi-3)\n",
        "\n",
        "Nesta etapa, carregamos o **modelo de linguagem Phi-3 Mini**, respons√°vel pela gera√ß√£o final das respostas.\n",
        "\n",
        "**Pontos importantes:**\n",
        "- O modelo √© carregado em **GPU** (`device_map=\"cuda\"`) quando dispon√≠vel\n",
        "- Utilizamos `float16` para reduzir uso de mem√≥ria\n",
        "- O cache √© desativado para evitar incompatibilidades do Phi-3 no Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    return_full_text=False,\n",
        "    use_cache=False  # Evita problemas de compatibilidade na gera√ß√£o\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Etapa 2 ‚Äî Imports do LangChain\n",
        "\n",
        "Aqui importamos as ferramentas do **LangChain** respons√°veis por:\n",
        "- Carregar documentos da web\n",
        "- Representar documentos de forma estruturada\n",
        "- Dividir textos grandes em chunks menores\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÑ Etapa 3 ‚Äî Fonte de Dados Local (TXT)\n",
        "\n",
        "Nesta etapa, carregamos um **arquivo de texto local**, simulando documentos internos como:\n",
        "- Pol√≠ticas\n",
        "- Manuais\n",
        "- FAQs\n",
        "\n",
        "O texto √© convertido em um objeto `Document`, que ser√° usado ao longo do pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with open(\"data/politica_reembolso.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    texto_arquivo = file.read()\n",
        "\n",
        "doc_txt = Document(\n",
        "    page_content=texto_arquivo,\n",
        "    metadata={\"source\": \"politica_reembolso.txt\"}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåê Etapa 4 ‚Äî Fonte de Dados Web\n",
        "\n",
        "Aqui carregamos conte√∫do diretamente da **web**, utilizando uma URL p√∫blica.\n",
        "\n",
        "Isso simula cen√°rios reais onde o sistema precisa responder com base em:\n",
        "- Not√≠cias\n",
        "- Blogs\n",
        "- Documenta√ß√£o online\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://www.bbc.com/portuguese/articles/cd19vexw0y1o\",)\n",
        ")\n",
        "\n",
        "docs_web = loader.load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Etapa 5 ‚Äî Unifica√ß√£o das Fontes\n",
        "\n",
        "Unificamos todas as fontes de dados em uma √∫nica lista de documentos.\n",
        "\n",
        "A partir deste ponto, o pipeline trata **todas as fontes de forma uniforme**, independentemente da origem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "docs = [doc_txt] + docs_web\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÇÔ∏è Etapa 6 ‚Äî Chunking dos Documentos\n",
        "\n",
        "Documentos grandes s√£o divididos em **chunks menores** para:\n",
        "- Melhorar a busca sem√¢ntica\n",
        "- Evitar estouro de contexto no LLM\n",
        "\n",
        "Utilizamos sobreposi√ß√£o (`overlap`) para preservar continuidade sem√¢ntica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Etapa 7 ‚Äî Embeddings + Vector Store (FAISS)\n",
        "\n",
        "Nesta etapa:\n",
        "- Cada chunk √© convertido em um **vetor num√©rico (embedding)**\n",
        "- Os vetores s√£o armazenados em um **√≠ndice FAISS**, permitindo busca sem√¢ntica eficiente\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "hf_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=hf_embeddings\n",
        ")\n",
        "\n",
        "print(\"Vector store criado com sucesso!\")\n",
        "print(f\"Total de vetores indexados: {vectorstore.index.ntotal}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîé Etapa 8 ‚Äî Retriever (Busca Sem√¢ntica)\n",
        "\n",
        "O **retriever** √© respons√°vel por buscar os chunks mais relevantes dado uma pergunta.\n",
        "\n",
        "Neste exemplo, usamos `k=1` para retornar apenas o chunk mais relevante, reduzindo ru√≠do e custo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "query = \"Qual √© a pol√≠tica de reembolso?\"\n",
        "docs_relevantes = retriever.invoke(query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Etapa 9 ‚Äî Prompt + Gera√ß√£o da Resposta (RAG)\n",
        "\n",
        "Na etapa final, combinamos:\n",
        "- üìÑ Contexto recuperado\n",
        "- ‚ùì Pergunta do usu√°rio\n",
        "- ü§ñ LLM\n",
        "\n",
        "O modelo gera a resposta **baseando-se explicitamente no contexto fornecido**, caracterizando o padr√£o RAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "<|begin_of_text|>\n",
        "<|start_header_id|>system<|end_header_id|>\n",
        "Use o contexto para responder a pergunta.\n",
        "Se n√£o souber, diga que n√£o sabe.\n",
        "<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "Contexto:\n",
        "{contexto}\n",
        "\n",
        "Pergunta:\n",
        "{pergunta}\n",
        "<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "contexto = docs_relevantes[0].page_content\n",
        "\n",
        "resposta = llm(\n",
        "    template.format(pergunta=query, contexto=contexto),\n",
        "    max_new_tokens=150\n",
        ")\n",
        "\n",
        "print(resposta[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ‚úÖ Conclus√£o\n",
        "\n",
        "Neste notebook, foi constru√≠do um **pipeline RAG completo**, integrando:\n",
        "- M√∫ltiplas fontes de dados\n",
        "- Busca sem√¢ntica\n",
        "- Gera√ß√£o de respostas condicionadas ao contexto\n",
        "\n",
        "Este padr√£o √© amplamente utilizado em **assistentes inteligentes**, **chatbots corporativos** e **sistemas de busca avan√ßados**.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}